{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8db0429-794d-48a9-a367-2e8d05976be2",
   "metadata": {},
   "source": [
    "# Spark Salting technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39beda94-3afa-4a28-b011-22af1528c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Salting technique\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2db4a8-85f3-4f82-a64a-067f865d04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create the example dataset of fact and dimesion we would use for demonstration\n",
    "# Python program to generate random Fact table data\n",
    "# [1, ,\"ORD1001\", \"D102\", 56]\n",
    "import random\n",
    "\n",
    "\n",
    "def generate_fact_data(counter=100):\n",
    "    fact_records = []\n",
    "    dim_keys = [\"D100\", \"D101\", \"D102\", \"D103\", \"D104\"]\n",
    "    order_ids = [\"ORD\" + str(i) for i in range(1001, 1010)]\n",
    "    qty_range = [i for i in range(10, 120)]\n",
    "    for i in range(counter):\n",
    "        _record = [i, random.choice(order_ids), random.choice(dim_keys), random.choice(qty_range)]\n",
    "        fact_records.append(_record)\n",
    "    return fact_records\n",
    "\n",
    "# We will generate 200 records with random data in Fact to create skewness\n",
    "fact_records = generate_fact_data(200)\n",
    "\n",
    "dim_records = [\n",
    "    [\"D100\", \"Product A\"],\n",
    "    [\"D101\", \"Product B\"],\n",
    "    [\"D102\", \"Product C\"],\n",
    "    [\"D103\", \"Product D\"],\n",
    "    [\"D104\", \"Product E\"]\n",
    "]\n",
    "\n",
    "_fact_cols = [\"id\", \"order_id\", \"prod_id\", \"qty\"]s\n",
    "_dim_cols = [\"prod_id\", \"prod_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ffb3da-39e7-42f1-83ef-284bb096bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Fact Data Frame\n",
    "fact_df = spark.createDataFrame(data = fact_records, schema=_fact_cols)\n",
    "\n",
    "fact_df.printSchema()\n",
    "fact_df.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbebb19-841f-48fe-8cad-277c93e01d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Prod Dim Data Frame\n",
    "dim_df = spark.createDataFrame(data = dim_records, schema=_dim_cols)\n",
    "\n",
    "dim_df.printSchema()\n",
    "dim_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c991eb7-a626-4d9c-b0c9-b56b69d6a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Spark parameters - We have to turn off AQL to demonstrate Salting\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "# Check the parameters\n",
    "print(spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d0d86-000b-46ba-8bcb-852da95d9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets join the fact and dim without salting\n",
    "joined_df = fact_df.join(dim_df, on=\"prod_id\", how=\"leftouter\")\n",
    "joined_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e19774-3eef-4f89-964a-daa413ed4f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count\n",
    "\n",
    "partition_df = joined_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(\"id\"))\n",
    "partition_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6cbe8-2ef3-4c57-8f3d-83ff44b36e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let prepare the salt\n",
    "import random\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# UDF to return a random number every time\n",
    "def rand(): return random.randint(0, 4) #Since we are distributing the data in 5 partitions\n",
    "rand_udf = udf(rand)\n",
    "\n",
    "# Salt Data Frame to add to dimension\n",
    "salt_df = spark.range(0, 5)\n",
    "salt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13fb4db-1d68-492a-99c7-1d0532fc5e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salted Fact\n",
    "from pyspark.sql.functions import lit, expr, concat\n",
    "\n",
    "salted_fact_df = fact_df.withColumn(\"salted_prod_id\", concat(\"prod_id\",lit(\"_\"), lit(rand_udf())))\n",
    "salted_fact_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4c87e-b86a-4d11-b3df-dd21c8395394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salted DIM\n",
    "salted_dim_df = dim_df.join(salt_df, how=\"cross\").withColumn(\"salted_prod_id\", concat(\"prod_id\", lit(\"_\"), \"id\")).drop(\"id\")\n",
    "\n",
    "salted_dim_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c5156-e61d-4199-8455-f107e3c4ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make the salted join now\n",
    "salted_joined_df = salted_fact_df.join(salted_dim_df, on=\"salted_prod_id\", how=\"leftouter\")\n",
    "salted_joined_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc77f98c-ddd8-495a-8a5f-9d80eb2e6224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count\n",
    "\n",
    "partition_df = salted_joined_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\") \\\n",
    "    .agg(count(lit(1)).alias(\"count\")).orderBy(\"partition_num\")\n",
    "\n",
    "partition_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749ff1e-2abc-4d15-b789-9257c4960335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
