{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b290be-a351-4bc6-a898-1341f50af784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"User Defined Functions\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861562f-75b1-4963-812e-96b2dec7d929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f41a3-f668-4458-a9c3-6753585953c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emp = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"hdfs://namenode:9000/input/data/employee_records.csv\")\n",
    "emp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b214d75-5ac5-40e6-ab74-54acf423e698",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a function to generate 10% of Salary as Bonus\n",
    "\n",
    "def bonus(salary):\n",
    "    return int(salary) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93265aa-5ef4-4362-8a56-5e6ad7b5f3ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Register as UDF\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "bonus_udf = F.udf(bonus)\n",
    "\n",
    "emp.withColumn(\"bonus\", bonus_udf(F.col(\"salary\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c32d9-4220-4a74-9752-34d28e9cde30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create new column as bonus using UDF\n",
    "from pyspark.sql.functions import expr\n",
    "spark.udf.register(\"bonus_sql_udf\", bonus, \"double\")\n",
    "emp.withColumn(\"bonus\", expr(\"bonus_sql_udf(salary)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8e6be-7973-4d44-922a-0672143fd511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create new column as bonus without UDF\n",
    "\n",
    "emp.withColumn(\"bonus\", expr(\"salary * 0.1\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17cd1c1-6650-44c4-a4ed-ba1a58a4dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1e6e1-3afb-47ee-922a-57c522621828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "emp_skew = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"hdfs://namenode:9000/input/data/employee_records_skewed.csv\")\n",
    "emp_skew = emp_skew.filter(F.col(\"salary\") < 25000).join(emp_skew.filter(F.col(\"salary\") < 25000), on=\"department_id\")\n",
    "emp_skew = emp_skew.coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4413067-8700-484e-95fa-2e77ba137a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register as UDF\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "dept_square = F.udf(lambda x: x**2)\n",
    "# Apply the UDF to the DataFrame (skewed), which should trigger a spill\n",
    "emp_skew.withColumn(\"dept_square\", dept_square(F.col(\"department_id\"))).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc45429-3896-4968-9cb5-e77b004cc1ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8c770-1dcd-44cb-b3e8-f0cdad80afca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
