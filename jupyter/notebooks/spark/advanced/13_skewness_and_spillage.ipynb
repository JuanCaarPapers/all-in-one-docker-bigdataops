{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0622da-9229-4822-8482-b9b76cd3b107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Optimizing Skewness and Spillage\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.cores.max\", 8)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99724543-fea0-4b89-996d-cf8cea168bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9191ed11-3ca0-4c15-a3ba-a80bc6fbf9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read Employee data\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/input/employee_records_skewed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d8df89-6d6a-4e10-99d5-560f8ea0b3fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read DEPT CSV data\n",
    "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "\n",
    "dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"/data/input/department_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba90ff0-a0dc-4a20-bd84-c2389d4ca147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join Datasets\n",
    "\n",
    "df_joined = emp.join(dept, on=emp.department_id==dept.department_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b613ce-a32a-44d4-bba2-1fa88753df0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d15de-b612-433b-8775-6f5f88b27da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explain Plan\n",
    "\n",
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50625a07-db99-4803-a39e-68707085bfbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count, lit\n",
    "\n",
    "part_df = df_joined.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
    "\n",
    "part_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d80db9-18c9-459e-a1a5-8b395184dde2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify Employee data based on department_id\n",
    "from pyspark.sql.functions import count, lit, desc, col\n",
    "\n",
    "emp.groupBy(\"department_id\").agg(count(lit(1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a7d44-5f4f-4301-9d4f-6e627334d529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set shuffle partitions to a lesser number - 16\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d20825-28f4-4fef-99a8-9c65a64f73e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let prepare the salt\n",
    "import random\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# UDF to return a random number every time and add to Employee as salt\n",
    "@udf\n",
    "def salt_udf():\n",
    "    return random.randint(0, 32)\n",
    "\n",
    "# Salt Data Frame to add to department\n",
    "salt_df = spark.range(0, 32)\n",
    "salt_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a51cc0a-43bd-4cba-8996-8eece7d97045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Salted Employee\n",
    "from pyspark.sql.functions import lit, concat\n",
    "\n",
    "salted_emp = emp.withColumn(\"salted_dept_id\", concat(\"department_id\", lit(\"_\"), salt_udf()))\n",
    "\n",
    "salted_emp.show()                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7f520-7b49-4473-9171-2b4281bbd3da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Salted Department\n",
    "\n",
    "salted_dept = dept.join(salt_df, how=\"cross\").withColumn(\"salted_dept_id\", concat(\"department_id\", lit(\"_\"), \"id\"))\n",
    "\n",
    "salted_dept.where(\"department_id = 9\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a09602-ccb0-44df-957e-a5ca7b193805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets make the salted join now\n",
    "salted_joined_df = salted_emp.join(salted_dept, on=salted_emp.salted_dept_id==salted_dept.salted_dept_id, how=\"left_outer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8c0bf-ee79-4f88-bf89-38a17c6d24a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfbd1c8-5617-4a4b-9c88-97de98079271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "salted_joined_df.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe5989-fe90-4d6b-9645-6534ba497354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the partition details to understand distribution\n",
    "from pyspark.sql.functions import spark_partition_id, count\n",
    "\n",
    "part_df = salted_joined_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
    "\n",
    "part_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a30325-65a3-4c65-a499-38147e42ce50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
