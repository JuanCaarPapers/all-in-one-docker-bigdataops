{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e662cf22-28a0-4533-b2b8-f8e1715fcada",
   "metadata": {},
   "source": [
    "# COUNT(1) vs COUNT(*) vs COUNT(COL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526666cf-8ac8-409f-89dd-512a6be00011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/26 11:14:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cbedac8cc1fe:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Count(1) vs Count(*)</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8441e4e6d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Count(1) vs Count(*)\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69801eee-aec0-47f9-96f4-6c2ccf671a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "|      transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "|2017-12-20 20:00:00| 561803551| 2077350195|Walgreen  ccd id:...|  69.66| 350411713|\n",
      "|2017-12-10 22:00:00| 966498100| 1232435973|Toys R Us   ccd i...|  16.44|1554564545|\n",
      "|2017-12-19 17:00:00|  40380012| 1898522855|Target   arc id: ...|2854.84| 920189167|\n",
      "|2017-12-19 19:00:00|1735489522|  847200066|unkn     ccd id: ...| 280.31|2011632272|\n",
      "|2017-01-11 14:00:00|1513345631| 1953761884|Home Depot   arc ...|  20.51|1528300441|\n",
      "|2017-12-24 23:00:00| 884145953| 2001148981|Costco  ccd id: 4...|  11.75|2116046074|\n",
      "|2017-05-14 19:00:00|1003554030| 1903529855|                unkn|  61.76|1710668653|\n",
      "|2017-12-24 19:00:00| 921164309|  847200066|unkn   ppd id: 95...|  27.83|2074005445|\n",
      "|2017-12-16 17:00:00| 549217139| 1070485878|          Amazon.com|  12.21| 516488916|\n",
      "|2017-11-26 19:00:00| 719363881|  847200066|unkn     ccd id: ...|  67.32|2055198208|\n",
      "|2017-12-09 18:00:00|  85469564|  151850986|unkn    arc id: 1...| 383.43| 675146248|\n",
      "|2017-11-28 21:00:00| 634593938| 2001148981|              Costco|1663.99|2031527420|\n",
      "|2017-12-22 18:00:00| 425891861| 1817581369|unkn     arc id: ...|   17.0|2065205159|\n",
      "|2017-12-18 12:00:00|1524523277|  860355551|   Sonic       12-19|   0.93|1410131357|\n",
      "|2017-08-30 18:00:00| 348239856| 1768219832|unkn     arc id: ...|  40.35| 481821583|\n",
      "|2017-11-23 20:00:00|1818459158| 1232435973|Toys R Us     arc...| 2472.7| 814348903|\n",
      "|2017-12-04 13:00:00| 389577000| 1647858807|Verizon Wireless ...|   71.9|1439918695|\n",
      "|2017-11-26 12:00:00|1096904243|  865681996|Nordstrom   ccd i...| 1666.2|1107275933|\n",
      "|2017-12-24 21:00:00| 442311947|  400404203|  CVS       Ashgabat|   0.73| 957346984|\n",
      "|2017-12-21 13:00:00|1124093234| 1076023740|Aldi  ccd id: 543...|  25.97|1296036143|\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets read the dataframe to check the data\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"orc\") \\\n",
    "    .load(\"hdfs://namenode:9000/input/data/sales_orc\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10464096-da89-498a-be92-45808a495330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['trx_id], ['trx_id, count(1) AS count(1)#205L]\n",
      "+- Relation [transacted_at#0,trx_id#1,retailer_id#2,description#3,amount#4,city_id#5] orc\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "trx_id: int, count(1): bigint\n",
      "Aggregate [trx_id#1], [trx_id#1, count(1) AS count(1)#205L]\n",
      "+- Relation [transacted_at#0,trx_id#1,retailer_id#2,description#3,amount#4,city_id#5] orc\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [trx_id#1], [trx_id#1, count(1) AS count(1)#205L]\n",
      "+- Project [trx_id#1]\n",
      "   +- Relation [transacted_at#0,trx_id#1,retailer_id#2,description#3,amount#4,city_id#5] orc\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[trx_id#1], functions=[count(1)], output=[trx_id#1, count(1)#205L])\n",
      "   +- Exchange hashpartitioning(trx_id#1, 200), ENSURE_REQUIREMENTS, [plan_id=381]\n",
      "      +- HashAggregate(keys=[trx_id#1], functions=[partial_count(1)], output=[trx_id#1, count#209L])\n",
      "         +- FileScan orc [trx_id#1] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[hdfs://namenode:9000/input/data/sales_orc], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<trx_id:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "spark.sparkContext.setJobDescription(\"save count(1)\")\n",
    "df.groupBy(\"trx_id\").agg(F.count(F.lit(1))).explain(True)\n",
    "df.groupBy(\"trx_id\").agg(F.count(F.lit(1))).write.format(\"noop\").mode(\"overwrite\").save()\n",
    "spark.sparkContext.setJobDescription(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "290e8312-4fe3-4f90-8725-005b25b88a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['trx_id], ['trx_id, count('city_id) AS count(city_id)#259]\n",
      "+- Relation [transacted_at#0,trx_id#1,retailer_id#2,description#3,amount#4,city_id#5] orc\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "trx_id: int, count(city_id): bigint\n",
      "Aggregate [trx_id#1], [trx_id#1, count(city_id#5) AS count(city_id)#259L]\n",
      "+- Relation [transacted_at#0,trx_id#1,retailer_id#2,description#3,amount#4,city_id#5] orc\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [trx_id#1], [trx_id#1, count(city_id#5) AS count(city_id)#259L]\n",
      "+- Project [trx_id#1, city_id#5]\n",
      "   +- Relation [transacted_at#0,trx_id#1,retailer_id#2,description#3,amount#4,city_id#5] orc\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[trx_id#1], functions=[count(city_id#5)], output=[trx_id#1, count(city_id)#259L])\n",
      "   +- Exchange hashpartitioning(trx_id#1, 200), ENSURE_REQUIREMENTS, [plan_id=499]\n",
      "      +- HashAggregate(keys=[trx_id#1], functions=[partial_count(city_id#5)], output=[trx_id#1, count#263L])\n",
      "         +- FileScan orc [trx_id#1,city_id#5] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[hdfs://namenode:9000/input/data/sales_orc], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<trx_id:int,city_id:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get count(col_name) performance\n",
    "spark.sparkContext.setJobDescription(\"save count(city_id)\")\n",
    "df.groupBy(\"trx_id\").agg(F.count(\"city_id\")).explain(True)\n",
    "df.groupBy(\"trx_id\").agg(F.count(\"city_id\")).write.format(\"noop\").mode(\"overwrite\").save()\n",
    "spark.sparkContext.setJobDescription(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "863ac0ef-7829-42b6-90a4-2f83d1b48dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['trx_id], ['trx_id, count(1) AS count(1)#287L]\n",
      "+- Relation [transacted_at#0,trx_id#1,retailer_id#2,description#3,amount#4,city_id#5] orc\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "trx_id: int, count(1): bigint\n",
      "Aggregate [trx_id#1], [trx_id#1, count(1) AS count(1)#287L]\n",
      "+- Relation [transacted_at#0,trx_id#1,retailer_id#2,description#3,amount#4,city_id#5] orc\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [trx_id#1], [trx_id#1, count(1) AS count(1)#287L]\n",
      "+- Project [trx_id#1]\n",
      "   +- Relation [transacted_at#0,trx_id#1,retailer_id#2,description#3,amount#4,city_id#5] orc\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[trx_id#1], functions=[count(1)], output=[trx_id#1, count(1)#287L])\n",
      "   +- Exchange hashpartitioning(trx_id#1, 200), ENSURE_REQUIREMENTS, [plan_id=558]\n",
      "      +- HashAggregate(keys=[trx_id#1], functions=[partial_count(1)], output=[trx_id#1, count#291L])\n",
      "         +- FileScan orc [trx_id#1] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[hdfs://namenode:9000/input/data/sales_orc], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<trx_id:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get count(*) performance\n",
    "spark.sparkContext.setJobDescription(\"save count(*)\")\n",
    "df.groupBy(\"trx_id\").agg(F.count(\"*\")).explain(True)\n",
    "df.groupBy(\"trx_id\").agg(F.count(\"*\")).write.format(\"noop\").mode(\"overwrite\").save()\n",
    "spark.sparkContext.setJobDescription(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd2d4b0f-1fca-4ca0-88c4-b58113cd662a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['trx_id], ['trx_id, count(1) AS count(1)#313L]\n",
      "+- Filter (month(cast(transacted_at#0 as date)) = 11)\n",
      "   +- Relation [transacted_at#0,trx_id#1,retailer_id#2,description#3,amount#4,city_id#5] orc\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "trx_id: int, count(1): bigint\n",
      "Aggregate [trx_id#1], [trx_id#1, count(1) AS count(1)#313L]\n",
      "+- Filter (month(cast(transacted_at#0 as date)) = 11)\n",
      "   +- Relation [transacted_at#0,trx_id#1,retailer_id#2,description#3,amount#4,city_id#5] orc\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [trx_id#1], [trx_id#1, count(1) AS count(1)#313L]\n",
      "+- Project [trx_id#1]\n",
      "   +- Filter (isnotnull(transacted_at#0) AND (month(cast(transacted_at#0 as date)) = 11))\n",
      "      +- Relation [transacted_at#0,trx_id#1,retailer_id#2,description#3,amount#4,city_id#5] orc\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[trx_id#1], functions=[count(1)], output=[trx_id#1, count(1)#313L])\n",
      "   +- Exchange hashpartitioning(trx_id#1, 200), ENSURE_REQUIREMENTS, [plan_id=621]\n",
      "      +- HashAggregate(keys=[trx_id#1], functions=[partial_count(1)], output=[trx_id#1, count#317L])\n",
      "         +- Project [trx_id#1]\n",
      "            +- Filter (isnotnull(transacted_at#0) AND (month(cast(transacted_at#0 as date)) = 11))\n",
      "               +- FileScan orc [transacted_at#0,trx_id#1] Batched: true, DataFilters: [isnotnull(transacted_at#0), (month(cast(transacted_at#0 as date)) = 11)], Format: ORC, Location: InMemoryFileIndex(1 paths)[hdfs://namenode:9000/input/data/sales_orc], PartitionFilters: [], PushedFilters: [IsNotNull(transacted_at)], ReadSchema: struct<transacted_at:timestamp,trx_id:int>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get filter + count(*) performance\n",
    "spark.sparkContext.setJobDescription(\"save filter-count(*)\")\n",
    "df.filter(F.month(F.col(\"transacted_at\")) == 11).groupBy(\"trx_id\").agg(F.count(\"*\")).explain(True)\n",
    "df.filter(F.month(F.col(\"transacted_at\")) == 11).groupBy(\"trx_id\").agg(F.count(\"*\")).write.format(\"noop\").mode(\"overwrite\").save()\n",
    "spark.sparkContext.setJobDescription(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2336c58b-32f3-4ada-bed7-db834417f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f16ee2-ff90-4dce-82fa-c20ab6de6f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
