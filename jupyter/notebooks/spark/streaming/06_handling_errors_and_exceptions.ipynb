{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a1693-6c4d-423f-a176-13c4345c885c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the Spark Session\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession \n",
    "    .builder \n",
    "    .appName(\"Handling errors and Exceptions\") \n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \n",
    "    .config('spark.jars.packages', f'org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark.__version__}')\n",
    "    .config('spark.jars', '/home/jovyan/postgresql.jar')\n",
    "    .config(\"spark.sql.shuffle.partitions\", 8)\n",
    "    .master(\"spark://spark-master:7077\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee70c4-9260-4bcf-82f1-acff3f39ce06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the kafka_df to read from kafka\n",
    "\n",
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"device-data\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fe3f43-bbb6-43dc-93db-f0ee921193e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined logic for handling the error records\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import from_json, col, expr, explode, current_timestamp, lit, size\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType\n",
    "\n",
    "def flatten_data(df):\n",
    "    \n",
    "    # Convert binary to string value column\n",
    "    kafka_json_df = df.withColumn(\"value\", expr(\"cast(value as string)\"))\n",
    "    \n",
    "    # Define Schema\n",
    "    json_schema = (\n",
    "        StructType(\n",
    "        [StructField('customerId', StringType(), True), \n",
    "        StructField('data', StructType(\n",
    "            [StructField('devices', \n",
    "                         ArrayType(StructType([ \n",
    "                            StructField('deviceId', StringType(), True), \n",
    "                            StructField('measure', StringType(), True), \n",
    "                            StructField('status', StringType(), True), \n",
    "                            StructField('temperature', LongType(), True)\n",
    "                        ]), True), True)\n",
    "            ]), True), \n",
    "        StructField('eventId', StringType(), True), \n",
    "        StructField('eventOffset', LongType(), True), \n",
    "        StructField('eventPublisher', StringType(), True), \n",
    "        StructField('eventTime', StringType(), True)\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Expand JSON from Value column using Schema\n",
    "    json_df = kafka_json_df.withColumn(\"values_json\", from_json(col(\"value\"), json_schema))\n",
    "    \n",
    "    # Filter out for error data\n",
    "    error_df = json_df.select(\"key\", \"value\").withColumn(\"eventtimestamp\",lit(current_timestamp())) \\\n",
    "        .where(\"values_json.customerId is null or size(values_json.data.devices) = 0\")\n",
    "    \n",
    "    # Filter out correct flattened data\n",
    "    streaming_df = json_df.where(\"values_json.customerId is not null and size(values_json.data.devices) > 0\") \\\n",
    "        .selectExpr(\"values_json.*\")\n",
    "    \n",
    "    # Explode the correct flattened data\n",
    "    exploded_df = streaming_df.withColumn(\"data_devices\", explode(\"data.devices\"))\n",
    "    \n",
    "    # Flatten data\n",
    "    flattened_df = (\n",
    "    exploded_df\n",
    "    .drop(\"data\")\n",
    "    .withColumn(\"deviceId\", col(\"data_devices.deviceId\"))\n",
    "    .withColumn(\"measure\", col(\"data_devices.measure\"))\n",
    "    .withColumn(\"status\", col(\"data_devices.status\"))\n",
    "    .withColumn(\"temperature\", col(\"data_devices.temperature\"))\n",
    "    .drop(\"data_devices\")\n",
    "    )\n",
    "\n",
    "    # Return both Flattened & Error Dataframe\n",
    "    return flattened_df, error_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc18c5b-ec95-4719-adff-5b9f349e625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write the dataframe to JDBC (Postgres)\n",
    "\n",
    "def postgres(df, table_name):\n",
    "    (\n",
    "\tdf.write\n",
    "\t.mode(\"append\")\n",
    "\t.format(\"jdbc\")\n",
    "\t.option(\"driver\", \"org.postgresql.Driver\")\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres:5432/streaming_db\")\n",
    "    .option(\"dbtable\", \"device_data\")\n",
    "    .option(\"user\", \"postgres\")\n",
    "    .option(\"password\", \"postgres\")\n",
    "\t.save()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aef5000-5ac0-46bd-90b1-3a13aec2e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Error and Exception and write to JDBC \n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def device_data_output(kafka_df, batch_id):\n",
    "    print(\"Batch id:\" + str(batch_id))\n",
    "    try:\n",
    "        # Get the Flattened and Error Dataframe\n",
    "        flattened_df, error_df_raw = flatten_data(kafka_df)\n",
    "\n",
    "        # Add the batchid column in Error Dataframe\n",
    "        error_df = error_df_raw.withColumn(\"batchid\", lit(batch_id))\n",
    "\n",
    "        # Write Flattened Dataframe to JDBC\n",
    "        postgres(flattened_df, \"device_data\")\n",
    "\n",
    "        # Write Error Datafram to JDBC\n",
    "        postgres(error_df, \"device_data_error\")\n",
    "\n",
    "        # Display both Dataframes for confirmation\n",
    "        flattened_df.show()\n",
    "        error_df.show()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        kafka_df.write.format(\"parquet\").mode(\"append\").save(\"hdfs://namenode:9000/output/streaming/06/device_data_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a2511-13c6-4edf-972e-51b4c0a0f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running foreachBatch\n",
    "\n",
    "(kafka_df\n",
    " .writeStream\n",
    " .foreachBatch(device_data_output)\n",
    " .trigger(processingTime='10 seconds')\n",
    " .option(\"checkpointLocation\", f\"/home/jovyan/streaming_checkpoint_dir/{spark.sparkContext.appName.replace(' ', '_')}\")\n",
    " .start()\n",
    " .awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf71619-2113-4c85-a84d-4fff55847c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
